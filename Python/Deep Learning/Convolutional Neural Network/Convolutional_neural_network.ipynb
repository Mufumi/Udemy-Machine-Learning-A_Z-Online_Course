{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Convolutional_neural_network.ipynb","provenance":[{"file_id":"1Y-a4g98w93GHswXLRLoiogvMYNPgzPE9","timestamp":1638875913195}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3DR-eO17geWu"},"source":["# Convolutional Neural Network\n","\n","This [tutorial](https://www.superdatascience.com/blogs/the-ultimate-guide-to-convolutional-neural-networks-cnn) contains an intuitive explanation of CNNs"]},{"cell_type":"markdown","metadata":{"id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","metadata":{"id":"sCV30xyVhFbE"},"source":["import tensorflow as tf\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuPQeZokl683"},"source":["Dataset is too large for uploading onto Colab. This notebook will be run with the dataset located locally."]},{"cell_type":"code","metadata":{"id":"FIleuCAjoFD8"},"source":["tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oxQxCBWyoGPE"},"source":["## Part 1 - Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"MvE-heJNo3GG"},"source":["### Preprocessing the Training set"]},{"cell_type":"markdown","metadata":{"id":"xzT8nnxjnVdz"},"source":["This is done to prevent overfitting of the model. Image transformations are going to be performed onto the training data. Image augmentation is the term that is commonly used. The [keras library](https://keras.io/api/preprocessing/image/) is great for exploring image pre-processing modules"]},{"cell_type":"code","metadata":{"id":"0koUcJMJpEBD"},"source":["'''\n","rescale is a feature scaller that scales each pixel by dividing it by 255\n","shear_range, zoom_range and horizontal_flip are image translators\n","'''\n","\n","#Creting an instance of the ImageDataGenerator class\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","#Applying the flow_from_directory method on the train_datagen object\n","\n","training_set = train_datagen.flow_from_directory('dataset/training_set',\n","                                                 target_size = (64, 64),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","metadata":{"id":"SH4WzfOhpKc3"},"source":["'''\n","images in the test set are only scaled. We don't want to transform them\n","'''\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","test_set = test_datagen.flow_from_directory('dataset/test_set',\n","                                            target_size = (64, 64),\n","                                            batch_size = 32,\n","                                            class_mode = 'binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"af8O4l90gk7B"},"source":["## Part 2 - Building the CNN"]},{"cell_type":"markdown","metadata":{"id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","metadata":{"id":"SAUt4UMPlhLS"},"source":["cnn = tf.keras.models.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u5YJj_XMl5LF"},"source":["### Step 1 - Convolution"]},{"cell_type":"code","metadata":{"id":"XPzPrMckl-hV"},"source":["'''\n","filter - number of feature detectors\n","kernel_size - size of one row of the feature detector\n","activation - we want to keep activation as a rectified linear unit during the hidden layers\n","'''\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3])) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tf87FpvxmNOJ"},"source":[" ### Step 2 - Pooling"]},{"cell_type":"code","metadata":{"id":"ncpqPl69mOac"},"source":["'''\n","A relatively small pooling parameter is used for finer image detail\n","'''\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","metadata":{"id":"i_-FZjn_m8gk"},"source":["'''\n","input_shape parameter is removed because convolution layer is not an input.\n","Another pooling layer is also added to improve robustness\n","'''\n","\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","metadata":{"id":"6AZeOGCvnNZn"},"source":["cnn.add(tf.keras.layers.Flatten())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","metadata":{"id":"8GtmUlLd26Nq"},"source":["'''\n","We add a dense layer of 128 neurons \n","'''\n","\n","cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","metadata":{"id":"1p_Zj1Mc3Ko_"},"source":["cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D6XkI90snSDl"},"source":["## Part 3 - Training the CNN"]},{"cell_type":"markdown","metadata":{"id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","metadata":{"id":"NALksrNQpUlJ"},"source":["cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","metadata":{"id":"XUj1W4PJptta"},"source":["cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","metadata":{"id":"gsSiWEJY1BPB"},"source":["import numpy as np\n","from keras.preprocessing import image\n","\n","#Loading test image\n","test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n","\n","#Transform image to array\n","test_image = image.img_to_array(test_image)\n","\n","#Expanding the dimensions of the array\n","test_image = np.expand_dims(test_image, axis = 0)\n","\n","#Obtaining the results\n","result = cnn.predict(test_image)\n","\n","#Check which set of images belong to which classification\n","training_set.class_indices\n","\n","'''\n","result has 2D representation. The first is the batch and the second is the prediction of the image\n","'''\n","\n","if result[0][0] == 1:\n","  prediction = 'dog'\n","else:\n","  prediction = 'cat'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ED9KB3I54c1i"},"source":["print(prediction)"],"execution_count":null,"outputs":[]}]}